{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be looking at how to make a sentiment analysis model using Reccurent Neural Networks in Tensorflow. RNNs are the go-to for most NLP tasks today. The big advantage of the RNN is that it is able to effectively use data from previous time steps. The most distinct difference from a traditional NN is that an RNN takes in a sequence of inputs (words in our case). You can contrast this to a typical CNN where youâ€™d have just a singular image as input. With an RNN, however, the input can be anywhere from a short sentence to a 5 paragraph essay. Additionally, the order of inputs in this sequence can largely affect how the weight matrices and hidden state vectors change during training. The hidden states, after training, will hopefully capture the information from the past (the previous time steps).\n",
    "\n",
    "Let's look at how we can create an RNN model in Tensorflow that classifies sentences as having positive or negative sentimnent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sentiment analysis task, we'll be classifying movie reviews from IMDB. To get this dataset, we'll be using the TFlearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tflearn.datasets import imdb\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,valid_portion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, trainY = train\n",
    "testX, testY = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Pretrained Word Vectors Through GloVe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before we go into RNNs, we need to obtain some pretrained word vectors. The idea behind word vectors is that we want to represent each word as a d dimensional vector. Each of them will be 50 dimensional. We're going to store the words and the corresponding vector in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'glove.6B.50d.txt' #This text file has to be in the same folder. You can download it from \n",
    "                              #here: http://nlp.stanford.edu/projects/glove/ \n",
    "numWords = 400000\n",
    "numDimensions = 50\n",
    "fin=open(filename,'r')\n",
    "wordList=[] #Just contains a list of all the words \n",
    "wordVectorMatrix = np.zeros((numWords + 1,numDimensions)) # numWords+1 x numDimensions matrix representing the word vector matrix\n",
    "wordVectorDictionary = collections.defaultdict(list)\n",
    "with open(filename) as inputfile:\n",
    "    index = 0\n",
    "    for line in inputfile:\n",
    "        split = line.split()\n",
    "        word = (split[0])\n",
    "        wordList.append(word)\n",
    "        listValues = split[1:]\n",
    "        listValues = map(float, listValues)\n",
    "        wordVectorMatrix[index] = listValues\n",
    "        wordVectorDictionary[word] = listValues\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at what our inputs and outputs are for this network. The input is going to be a sentence of max length 20 words, and the output will be a single number representing the sentiment of the sentence (1 for extremely positive and 0 for extremely negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "numClasses = 2\n",
    "max_seq_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data = tf.placeholder(tf.float32, [None, max_seq_length,numDimensions]) #Number of examples, number of input, dimension of each input\n",
    "target = tf.placeholder(tf.float32, [None, numClasses])\n",
    "num_hidden = 24\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_hidden,state_is_tuple=True)\n",
    "val, _ = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden, int(target.get_shape()[1])]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[target.get_shape()[1]]))\n",
    "prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "cross_entropy = -tf.reduce_sum(target * tf.log(tf.clip_by_value(prediction,1e-10,1.0)))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 50)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Our test sentence is going to be \"I thought the movie was incredible and inspiring\"\n",
    "xTrain = np.zeros((1, max_seq_length, numDimensions)) # 10 x 50 matrix describing the input sentence\n",
    "xTrain[0][0] = wordVectorDictionary[\"i\"]\n",
    "xTrain[0][1] = wordVectorDictionary[\"thought\"]\n",
    "xTrain[0][2] = wordVectorDictionary[\"the\"]\n",
    "xTrain[0][3] = wordVectorDictionary[\"movie\"]\n",
    "xTrain[0][4] = wordVectorDictionary[\"was\"]\n",
    "xTrain[0][5] = wordVectorDictionary[\"incredible\"]\n",
    "xTrain[0][6] = wordVectorDictionary[\"and\"]\n",
    "xTrain[0][7] = wordVectorDictionary[\"inspiring\"]\n",
    "#xTrain[8] and xTrain[9] are going to just be filled with zeros\n",
    "print xTrain.shape\n",
    "yTrain = np.zeros((1, numClasses))\n",
    "yTrain[0] = 1\n",
    "print yTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "tf.summary.scalar('CrossEntropy_Loss', cross_entropy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-b038af405f34>:1 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch  0\n",
      "Epoch  1\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n",
      "Epoch  10\n",
      "Epoch  11\n",
      "Epoch  12\n",
      "Epoch  13\n",
      "Epoch  14\n",
      "Epoch  15\n",
      "Epoch  16\n",
      "Epoch  17\n",
      "Epoch  18\n",
      "Epoch  19\n",
      "Epoch  20\n",
      "Epoch  21\n",
      "Epoch  22\n",
      "Epoch  23\n",
      "Epoch  24\n",
      "Epoch  25\n",
      "Epoch  26\n",
      "Epoch  27\n",
      "Epoch  28\n",
      "Epoch  29\n",
      "Epoch  30\n",
      "Epoch  31\n",
      "Epoch  32\n",
      "Epoch  33\n",
      "Epoch  34\n",
      "Epoch  35\n",
      "Epoch  36\n",
      "Epoch  37\n",
      "Epoch  38\n",
      "Epoch  39\n",
      "Epoch  40\n",
      "Epoch  41\n",
      "Epoch  42\n",
      "Epoch  43\n",
      "Epoch  44\n",
      "Epoch  45\n",
      "Epoch  46\n",
      "Epoch  47\n",
      "Epoch  48\n",
      "Epoch  49\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "epoch = 50\n",
    "for i in range(epoch):\n",
    "    summary = sess.run(merged,{data: xTrain, target: yTrain})\n",
    "    writer.add_summary(summary, i)\n",
    "    train_step.run(session=sess,feed_dict={data: xTrain, target: yTrain})\n",
    "    print \"Epoch \",str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
